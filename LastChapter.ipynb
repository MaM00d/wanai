{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "c9AXsMSZl5"
      },
      "source": [],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "NgWEZicMYY"
      },
      "source": [
        "# lOAD QUANTIZED MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "cgdY3m6XoC"
      },
      "source": [
        "AceGptModelName = \"FreedomIntelligence/AceGPT-13B\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "xaIkSdCqVo"
      },
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(AceGptModelName)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    AceGptModelName, quantization_config=quantization_config, device_map={\"\": 0}\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "oF82z51kT5"
      },
      "source": [
        "# LOAD NORMAL MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "C6MK0rJytB"
      },
      "source": [
        "# normal_model = AutoModelForCausalLM.from_pretrained(\n",
        "#     AceGptModelName\n",
        "# )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "MOIKuoKUEz"
      },
      "source": [
        "# Load Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "FlZvcdFzEd"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(AceGptModelName)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "wCoJ7Ypu7N"
      },
      "source": [
        "# Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "USy69Uu1t5"
      },
      "source": [
        "## Old Prompt After Adding Context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "28yidCEtCe"
      },
      "source": [
        "def create_prompt(context, history ,patient, doctor):\n",
        "    prompt_template = (\n",
        "        f\"### Context\\n{context}\\n{history}\\n\\n### PATIENT\\n{patient}\\n\\n### DOCTOR\\n{doctor}</s>\"\n",
        "    )\n",
        "    return prompt_template"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "xqSx0cprsq"
      },
      "source": [
        "# Loading Datasets\n",
        "## spliting test and train datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "ljsPV7YxHJ"
      },
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"csv\", data_files=\"./datasets/large_dataset.csv\",split='train')\n",
        "dataset=dataset.train_test_split(test_size=0.3)\n",
        "dataset = dataset.map(lambda samples: tokenizer(create_prompt(\"\",samples['history'], samples['patient'], samples['doctor'])))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "qioYjjzgvX"
      },
      "source": [
        "# Training  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "XnkmnXVf6G"
      },
      "source": [
        "## Set Model in Training Mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "hGGt31gRO8"
      },
      "source": [
        "train_model = model\n",
        "train_model.train() # model in training mode (dropout modules are activated)\n",
        "\n",
        "# enable gradient check pointing\n",
        "train_model.gradient_checkpointing_enable()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "o5JzsHriIN"
      },
      "source": [
        "### enable quantized training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "iVvPzQFh7Q"
      },
      "source": [
        "from peft.utils.other import prepare_model_for_kbit_training\n",
        "from peft.mapping import get_peft_model\n",
        "from peft.config import LoraConfig \n",
        "\n",
        "train_model = prepare_model_for_kbit_training(train_model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# LoRA trainable version of model\n",
        "train_model = get_peft_model(train_model, config)\n",
        "\n",
        "# trainable parameter count\n",
        "train_model.print_trainable_parameters()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "pjp5RSawVQ"
      },
      "source": [
        "## Setting Training Args"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "dTGapDXWvU"
      },
      "source": [
        "from transformers import TrainingArguments\n",
        "# hyperparameters\n",
        "lr = 2e-4\n",
        "batch_size = 4\n",
        "num_epochs = 50\n",
        "output_dir= \"wanasgpt-ft\"\n",
        "\n",
        "# define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir ,\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    weight_decay=0.01,\n",
        "    logging_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=2,\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "gpd4Y6Ezwm"
      },
      "source": [
        "## Input Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "Vr8LqCHfEQ"
      },
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "if tokenizer.pad_token_id is None:\n",
        "  tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "if tokenizer.model_max_length > 100_000:\n",
        "  tokenizer.model_max_length = 2048\n",
        "\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "3Q5AMUz7fq"
      },
      "source": [
        "## Make trainer Object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "eIRPXquB3e"
      },
      "source": [
        "from transformers import Trainer\n",
        "trainer = Trainer(\n",
        "    model=train_model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "     eval_dataset=dataset[\"test\"],\n",
        "    args=training_args,\n",
        "     data_collator=data_collator\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "vHZzmdU8Ol"
      },
      "source": [
        "Start Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "5V9PlMScSK"
      },
      "source": [
        "# train model\n",
        "# model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# renable warnings\n",
        "# model.config.use_cache = True"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "Tjn521kGVM"
      },
      "source": [
        "#Model Using"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "LrJWcimgC9"
      },
      "source": [
        "from peft import LoraConfig \n",
        "lora_config = LoraConfig.from_pretrained(\n",
        "    output_dir + \"wanas-finetuned\"\n",
        ")\n",
        "peft_model = get_peft_model(model, lora_config)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "BaFBVgPlLF"
      },
      "source": [
        "text = create_prompt(\"\",\"\", \"\u0627\u0647\u0644\u0627 \u064a\u0627 \u062f\u0643\u062a\u0648\u0631\", \"\")\n",
        "device = \"cuda:0\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "outputs = peft_model.generate(**inputs, max_new_tokens=50)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "mvGx7zj4Xo"
      },
      "source": [
        "!pip install --upgrade huggingface-hub\n",
        "!pip install --upgrade transformers\n",
        "# get your account token from https://huggingface.co/settings/tokens\n",
        "token = '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'\n",
        "# import the relavant libraries for loggin in\n",
        "from huggingface_hub import HfApi, HfFolder\n",
        "\n",
        "# set api for login and save token\n",
        "api=HfApi()\n",
        "api.set_access_token(token)\n",
        "folder = HfFolder()\n",
        "folder.save_token(token)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "uLA60Y96Ri"
      },
      "source": [
        "model.push_to_hub(\"my-awesome-model\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "python",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}