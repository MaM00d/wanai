{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "c9AXsMSZl5"
      },
      "source": [],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "NgWEZicMYY"
      },
      "source": [
        "# lOAD QUANTIZED MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "cgdY3m6XoC"
      },
      "source": [
        "AceGptModelName = \"FreedomIntelligence/AceGPT-13B\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "xaIkSdCqVo"
      },
      "source": [
        "from transformers import BitsAndBytesConfig,AutoTokenizer, AutoModelForCausalLM\n",
        "from torch import bfloat16\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=bfloat16,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(AceGptModelName)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    AceGptModelName, quantization_config=quantization_config, device_map={\"\": 0}\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "XRXZs9dyCC"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(AceGptModelName)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "auHRX0BOfW"
      },
      "source": [
        "def create_prompt(context, history ,patient, doctor):\n",
        "    prompt_template = (\n",
        "        f\"### Context\\n{context}\\n{history}\\n\\n### PATIENT\\n{patient}\\n\\n### DOCTOR\\n{doctor}</s>\"\n",
        "    )\n",
        "    return prompt_template"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "HKORVZfg1n"
      },
      "source": [
        "#Model Using"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "LrJWcimgC9"
      },
      "source": [
        "from peft import LoraConfig \n",
        "from peft.mapping import get_peft_model\n",
        "lora_config = LoraConfig.from_pretrained(\n",
        "\"models/parm1\"\n",
        ")\n",
        "peft_model = get_peft_model(model, lora_config)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "bfYw1fZjo3"
      },
      "source": [
        "## PDFProcessor Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "xuPB8E5Mlp"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import PyPDF2\n",
        "# folder_path= 'phy/'\n",
        "class PDFProcessor:\n",
        "    def __init__(self, folder_path):\n",
        "        self.folder_path = folder_path\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_text(text):\n",
        "        page_number_pattern = r'\\bPage\\b\\s*\\d+|\\b\\d+\\b\\s*(?:/\\s*\\d+)?'\n",
        "        square_bracket_pattern = r'\\[.*?\\]'\n",
        "        url_pattern = r'http[s]?://\\S+|www\\.\\S+'\n",
        "        text = re.sub(page_number_pattern, '', text, flags=re.IGNORECASE)\n",
        "        text = re.sub(square_bracket_pattern, '', text)\n",
        "        text = re.sub(url_pattern, '', text)\n",
        "        return text\n",
        "\n",
        "    def read_pdfs_in_folder(self):\n",
        "        concatenated_text = \"\"\n",
        "        for filename in os.listdir(self.folder_path):\n",
        "            if filename.endswith('.pdf'):\n",
        "                file_path = os.path.join(self.folder_path, filename)\n",
        "                text_output = ''\n",
        "                with open(file_path, 'rb') as pdf_object:\n",
        "                    pdf_reader = PyPDF2.PdfReader(pdf_object)\n",
        "                    for page in pdf_reader.pages:\n",
        "                        text_output += page.extract_text()\n",
        "                concatenated_text += text_output + \" \"\n",
        "        return concatenated_text"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "WBmp3KxdVu"
      },
      "source": [
        "## TextEmbedder Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "Ktur6VjYvQ"
      },
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "import pandas as pd\n",
        "\n",
        "class TextEmbedder:\n",
        "    def __init__(self, model_name, model_kwargs=None, encode_kwargs=None):\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=model_name,\n",
        "            model_kwargs=model_kwargs,\n",
        "            encode_kwargs=encode_kwargs\n",
        "        )\n",
        "\n",
        "    def create_chunks(self, text):\n",
        "        text_splitter = SemanticChunker(\n",
        "            embeddings=self.embeddings,\n",
        "            breakpoint_threshold_type=\"percentile\",\n",
        "        )\n",
        "        docs = text_splitter.create_documents([text])\n",
        "        return [doc.page_content for doc in docs]\n",
        "\n",
        "    def save_chunks_to_csv(self, chunks, output_file):\n",
        "        df = pd.DataFrame({'chunks': chunks})\n",
        "        df.to_csv(output_file, encoding='utf-8', index=False)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "BGcuFwdEI3"
      },
      "source": [
        "## ChromaStore Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "Kui5XKZr7r"
      },
      "source": [
        "from langchain.schema.document import Document\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "\n",
        "class ChromaStore:\n",
        "    def __init__(self, persist_directory, embedding_function):\n",
        "        self.persist_directory = persist_directory\n",
        "        self.embedding_function = embedding_function\n",
        "        self.db = Chroma(persist_directory=persist_directory, embedding_function=embedding_function)\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_chunk_ids(chunks):\n",
        "        last_page_id = None\n",
        "        current_chunk_index = 0\n",
        "        for chunk in chunks:\n",
        "            source = chunk.metadata.get(\"source\")\n",
        "            page = chunk.metadata.get(\"page\")\n",
        "            current_page_id = f\"{source}:{page}\"\n",
        "            if current_page_id == last_page_id:\n",
        "                current_chunk_index += 1\n",
        "            else:\n",
        "                current_chunk_index = 0\n",
        "            chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
        "            last_page_id = current_page_id\n",
        "            chunk.metadata[\"id\"] = chunk_id\n",
        "        return chunks\n",
        "\n",
        "    def add_to_chroma(self, chunks: list[Document]):\n",
        "        chunks_with_ids = self.calculate_chunk_ids(chunks)\n",
        "        existing_items = self.db.get(include=[])\n",
        "        existing_ids = set(existing_items[\"ids\"])\n",
        "        new_chunks = [chunk for chunk in chunks_with_ids if chunk.metadata[\"id\"] not in existing_ids]\n",
        "        if new_chunks:\n",
        "            new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
        "            self.db.add_documents(new_chunks, ids=new_chunk_ids)\n",
        "            self.db.persist()\n",
        "\n",
        "    def query_rag(self, query_text, k=5):\n",
        "        results = self.db.similarity_search_with_score(query_text, k)\n",
        "        return results"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "tS6zj3lw1d"
      },
      "source": [
        "# RAG main Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "bVO2BC41gk"
      },
      "source": [
        "folder_path = 'phy/'\n",
        "pdf_processor = PDFProcessor(folder_path)\n",
        "all_text = pdf_processor.read_pdfs_in_folder()\n",
        "all_text = PDFProcessor.clean_text(all_text)\n",
        "\n",
        "# Step 2: Embed and Chunk Text\n",
        "model_name = \"asafaya/bert-medium-arabic\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "text_embedder = TextEmbedder(model_name, model_kwargs, encode_kwargs)\n",
        "chunks = text_embedder.create_chunks(all_text)\n",
        "text_embedder.save_chunks_to_csv(chunks, 'rag.csv')\n",
        "\n",
        "documents = []\n",
        "for chunk in chunks:\n",
        "    # Provide appropriate page_content for each chunk\n",
        "    document = Document(page_content=chunk, metadata={\"source\": \"source_value\", \"page\": \"page_value\"})\n",
        "    documents.append(document)\n",
        "    \n",
        "# Step 3: Add Chunks to Chroma and Query\n",
        "chroma_store = ChromaStore(\"chroma\", text_embedder.embeddings)\n",
        "chroma_store.add_to_chroma(documents)\n",
        "# query_results = chroma_store.query_rag(\"\u0627\u0646\u0627 \u0645\u0634 \u062d\u0627\u0628\u0628 \u0634\u0643\u0644\u064a \u0627\u0639\u0645\u0644 \u0627\u064a\u0647 \u064a\u0627 \u062f\u0643\u062a\u0648\u0631 \u061f\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "BaFBVgPlLF"
      },
      "source": [
        "class ai:\n",
        "    def __init__(self):\n",
        "        self.history=\"\"\n",
        "    def run(self,text):\n",
        "        query_results = chroma_store.query_rag(text)\n",
        "        text = create_prompt(query_results,self.history, text, \"\")\n",
        "        self.history= self.history + text\n",
        "        device = \"cuda:0\"\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "        outputs = peft_model.generate(**inputs, max_new_tokens=50)\n",
        "        print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "    def clear_history(self):\n",
        "        self.history=\"\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "ERza08Sxaz"
      },
      "source": [
        "chat = ai()\n",
        "chat.run(\"hello\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "qY1DXY2pFp"
      },
      "source": [
        "chat.clear_history()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "eQTo7bddFR"
      },
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "python",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}